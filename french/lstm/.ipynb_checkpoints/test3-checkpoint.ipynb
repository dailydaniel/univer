{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../All_irregular_verb_list.csv', sep=';', encoding='utf-8')\n",
    "df = df[['First', 'Second', 'Third']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in df.columns[1:]:\n",
    "    df[column] = df[column].apply(lambda x: re.sub(r'[/.*]', '', x))\n",
    "    df[column] = df[column].apply(lambda x: re.sub(r'[^a-zA-Z]', '', x))\n",
    "\n",
    "criteria = df['First'].map(lambda x: len(set(x) & set(string.ascii_uppercase)) == 0)\n",
    "df = df[criteria]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_emb = {char: i+1 for i, char in enumerate(string.ascii_lowercase)}\n",
    "char_emb['0'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder = {val: key for key, val in char_emb.items()}\n",
    "\n",
    "def decode(idx, decoder):\n",
    "    t = ''.join([decoder[ix] for ix in idx])\n",
    "    return re.sub(r'[0.*]', '', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for column_id in range(len(df.columns)):\n",
    "    for i in range(df.index.size-1):\n",
    "        if len(df[df.columns[column_id]][i]) > max_len:\n",
    "            max_len = len(df[df.columns[column_id]][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First\n",
      "Second\n",
      "Third\n"
     ]
    }
   ],
   "source": [
    "def word2emb(emb, n, word):\n",
    "    res = np.zeros(len(word)+1, dtype=int)\n",
    "    for i, char in enumerate(word):\n",
    "        res[i] = emb[char]\n",
    "    return res\n",
    "\n",
    "for column in df.columns:\n",
    "    print(column)\n",
    "    df[column] = df[column].apply(lambda word: word2emb(char_emb, max_len, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, labels = df['First'], df['Second']\n",
    "data, labels = data.map(lambda x: torch.from_numpy(x)), labels.map(lambda x: torch.from_numpy(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(write_res = False):\n",
    "    accuracy = []\n",
    "    for sentence, target in zip(data, labels):\n",
    "        with torch.no_grad():\n",
    "            tag_scores = model(sentence)\n",
    "            _, preds = torch.max(tag_scores, 1)\n",
    "            first = decode(sentence.tolist(), decoder)\n",
    "            gt = decode(target.tolist(), decoder)\n",
    "            pred = decode(preds.tolist(), decoder)\n",
    "            accuracy.append(int(gt == pred))\n",
    "            if write_res:\n",
    "                print('{0} => gt: {1}, pred: {2}'.format(first, \n",
    "                                                         gt, \n",
    "                                                         pred))\n",
    "    return sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 8\n",
    "HIDDEN_DIM = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMVerbFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(LSTMVerbFormer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True) # False\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, vocab_size) # without * 2\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.char_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LSTMVerbFormer(EMBEDDING_DIM, HIDDEN_DIM, len(char_emb))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 9, 4, 5, 0]), torch.Size([6]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence, sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = model.char_embeddings(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3672, -0.4354,  0.7451, -0.4180, -0.3946,  0.0936, -2.1803, -0.2202],\n",
       "         [ 1.4989, -1.5685, -0.1214, -0.1095,  0.8053,  0.9983, -0.3050,  1.8419],\n",
       "         [-0.3616, -0.2323, -0.2106,  1.1162, -0.4773, -0.8742,  0.0860,  0.8378],\n",
       "         [-0.0238,  1.3686, -0.4056,  0.1771, -0.4261, -0.1113, -0.7985,  0.8763],\n",
       "         [ 0.4645, -0.2100,  1.2180,  1.4324,  0.4759, -0.2477, -0.0648, -1.0911],\n",
       "         [ 2.0399,  0.3315,  0.0111,  0.2345,  0.9206, -1.1231, -1.2478, -0.3788]],\n",
       "        grad_fn=<EmbeddingBackward>), torch.Size([6, 8]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb, emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_out, hidden = model.lstm(emb.view(len(sentence), 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0693, -0.0276, -0.0698, -0.0864,  0.0221, -0.1289, -0.0878,\n",
       "            0.0068, -0.2516, -0.0443, -0.3045, -0.3029,  0.2695,  0.0874,\n",
       "           -0.0336, -0.1518]],\n",
       " \n",
       "         [[-0.0219, -0.2673, -0.0703, -0.0205, -0.0906, -0.1983, -0.0078,\n",
       "            0.1578, -0.1488, -0.0532, -0.5636, -0.1503,  0.5515,  0.0276,\n",
       "           -0.2562, -0.1013]],\n",
       " \n",
       "         [[-0.1151, -0.2135, -0.0831, -0.1586, -0.0902, -0.2658, -0.0446,\n",
       "            0.1898, -0.1841, -0.1462, -0.2891, -0.1820,  0.2523,  0.1579,\n",
       "            0.0026, -0.1990]],\n",
       " \n",
       "         [[-0.0638, -0.1291, -0.1815, -0.1643,  0.0434, -0.2744, -0.0081,\n",
       "            0.0605, -0.3010, -0.1290, -0.1222, -0.2602,  0.2243,  0.0991,\n",
       "            0.0292, -0.1828]],\n",
       " \n",
       "         [[-0.1015,  0.0278, -0.0571, -0.0067, -0.0856, -0.2445, -0.0079,\n",
       "            0.1692, -0.0815, -0.3574, -0.0869, -0.3370,  0.1224,  0.1942,\n",
       "            0.0636, -0.1063]],\n",
       " \n",
       "         [[-0.0658,  0.1261, -0.1430,  0.1295, -0.2216, -0.3495, -0.0187,\n",
       "            0.1000, -0.0826, -0.2429,  0.0081, -0.1903,  0.1813,  0.0138,\n",
       "            0.0270, -0.0983]]], grad_fn=<CatBackward>), torch.Size([6, 1, 16]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out, lstm_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0658,  0.1261, -0.1430,  0.1295, -0.2216, -0.3495, -0.0187,\n",
       "            0.1000]],\n",
       " \n",
       "         [[-0.2516, -0.0443, -0.3045, -0.3029,  0.2695,  0.0874, -0.0336,\n",
       "           -0.1518]]], grad_fn=<StackBackward>), torch.Size([2, 1, 8]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0], hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2116,  0.1640, -0.3600,  0.2645, -0.3159, -0.7468, -0.0569,\n",
       "            0.1549]],\n",
       " \n",
       "         [[-0.3817, -0.1012, -0.7857, -0.5506,  0.4594,  0.1119, -0.0835,\n",
       "           -0.2977]]], grad_fn=<StackBackward>), torch.Size([2, 1, 8]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[1], hidden[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Parameter containing:\n",
       "  tensor([[-1.8689e-01, -3.4608e-01,  3.2576e-01,  1.1753e-01, -3.5274e-01,\n",
       "            4.1254e-02,  4.1443e-02, -7.7751e-02],\n",
       "          [ 1.8753e-01,  3.0312e-01, -2.9139e-01, -9.4925e-02,  9.1154e-02,\n",
       "           -1.6511e-01, -3.4213e-01,  3.4725e-01],\n",
       "          [ 6.6944e-02,  2.2907e-01, -2.6003e-02,  3.4659e-01,  2.9983e-01,\n",
       "           -2.9050e-01,  3.1816e-01,  2.3159e-01],\n",
       "          [-2.5763e-01,  8.8137e-02,  2.1695e-01,  2.6021e-01,  5.5331e-02,\n",
       "           -3.4053e-01,  2.2657e-01, -3.0851e-01],\n",
       "          [-1.9915e-01, -2.0600e-01, -1.2154e-01,  4.6871e-02,  3.4490e-01,\n",
       "           -1.5106e-01,  2.5568e-01,  3.1827e-02],\n",
       "          [ 2.3544e-01,  2.4281e-02,  7.5151e-02,  2.2761e-01, -2.7077e-02,\n",
       "           -2.0442e-02, -2.3380e-01, -2.2475e-01],\n",
       "          [-9.7622e-02,  2.0729e-01,  3.1938e-03,  1.6434e-01,  2.5890e-01,\n",
       "            2.0603e-01, -8.3834e-02,  2.3760e-01],\n",
       "          [-5.9033e-02,  9.5119e-02,  1.1279e-01, -2.6073e-01, -2.9193e-01,\n",
       "            1.0338e-01,  2.6099e-01,  3.9487e-02],\n",
       "          [ 1.2424e-01, -2.8920e-01, -7.0677e-02,  3.3404e-01,  2.3087e-01,\n",
       "            1.2540e-01, -2.9131e-01, -2.0367e-01],\n",
       "          [ 1.2938e-01, -2.7267e-01, -4.2390e-02, -3.4132e-02,  1.5251e-01,\n",
       "            3.4575e-02,  1.3745e-01,  1.2477e-01],\n",
       "          [-3.4462e-01,  1.1222e-01, -5.4159e-02,  3.4495e-02,  2.8031e-01,\n",
       "            1.3374e-01, -1.4759e-01,  9.0247e-02],\n",
       "          [-1.0443e-01,  3.1322e-01, -3.0662e-01,  1.1704e-01, -2.0849e-01,\n",
       "           -8.9691e-02, -1.8348e-01, -9.3140e-02],\n",
       "          [ 1.0529e-01, -1.1355e-01,  3.1129e-01, -5.0747e-02,  1.2844e-01,\n",
       "            1.0348e-01, -3.4566e-01, -9.0439e-02],\n",
       "          [-1.3345e-01,  2.6575e-01,  2.9122e-01,  2.1724e-01,  1.0031e-01,\n",
       "           -2.3450e-01, -1.6330e-02, -4.2914e-02],\n",
       "          [ 4.0793e-02, -1.8232e-01, -2.1333e-02, -1.2288e-01, -2.3992e-01,\n",
       "           -8.2558e-02,  2.2489e-01,  6.9359e-02],\n",
       "          [ 2.9175e-01,  2.2942e-01, -5.1601e-03,  3.4378e-01, -2.3016e-01,\n",
       "           -7.5398e-02, -1.4454e-01, -6.8544e-03],\n",
       "          [-1.8936e-01, -3.1540e-02, -1.2627e-01, -4.1010e-02,  1.4937e-01,\n",
       "            2.2284e-02, -2.3726e-01, -4.6675e-02],\n",
       "          [ 2.2113e-01,  6.2781e-02,  2.4500e-01, -2.2935e-01, -2.3771e-01,\n",
       "           -4.7450e-02,  2.3163e-01, -3.0702e-01],\n",
       "          [-6.9240e-02, -1.7109e-01, -3.2001e-02,  2.0484e-01, -3.1704e-01,\n",
       "           -2.5851e-01,  3.0055e-01, -2.8109e-01],\n",
       "          [ 3.3537e-01,  1.9380e-01, -1.9811e-02,  9.9491e-02,  1.4785e-01,\n",
       "            1.5869e-02,  1.1704e-02, -1.3653e-01],\n",
       "          [-2.8278e-01,  3.3686e-01,  1.5230e-01,  1.9188e-01, -1.4123e-01,\n",
       "            1.6827e-01, -2.7598e-01,  1.8149e-01],\n",
       "          [-3.3133e-01,  1.3966e-01,  1.4710e-01, -1.1035e-01,  2.1212e-01,\n",
       "           -3.5046e-01,  2.8877e-01, -3.4336e-02],\n",
       "          [ 1.0924e-02,  1.6365e-01, -1.3382e-01,  3.2971e-01,  2.5729e-01,\n",
       "            2.3492e-01,  9.1527e-03,  1.1529e-01],\n",
       "          [-2.7513e-02, -3.0103e-01, -3.4999e-02,  3.2474e-01,  1.4028e-01,\n",
       "            2.1082e-01,  7.5393e-02,  7.8538e-02],\n",
       "          [-1.5303e-01, -3.0661e-04, -2.0492e-01,  3.9903e-02, -2.3205e-01,\n",
       "            2.2369e-01,  5.7536e-02,  2.7972e-01],\n",
       "          [ 3.0152e-01, -3.1432e-01,  1.7884e-02,  2.4610e-01,  2.0812e-01,\n",
       "           -2.1260e-01,  7.4123e-02,  5.9898e-02],\n",
       "          [-2.9227e-01,  1.1466e-01,  1.5505e-02,  3.3595e-01, -1.5049e-01,\n",
       "           -3.3455e-01, -1.2899e-01, -1.3051e-01],\n",
       "          [-3.3326e-01,  1.5234e-01, -5.7969e-02, -2.3143e-01, -1.4251e-02,\n",
       "           -3.4136e-01, -1.8022e-01,  1.0549e-01],\n",
       "          [ 2.1087e-01,  3.3802e-01,  1.9715e-01,  1.6427e-01,  5.0631e-03,\n",
       "           -3.1607e-01,  3.2266e-01, -2.6987e-01],\n",
       "          [ 8.6614e-02,  1.2629e-01, -1.5034e-01,  2.6270e-01, -2.7854e-01,\n",
       "           -2.9551e-01,  2.1206e-01,  2.8579e-01],\n",
       "          [-2.8633e-02, -3.2497e-01, -2.4984e-01,  1.3195e-01, -6.5218e-02,\n",
       "            1.1842e-01,  2.9243e-01,  2.2137e-01],\n",
       "          [ 2.0137e-01, -1.4503e-01,  8.5191e-02,  1.6266e-01,  2.1664e-01,\n",
       "           -1.6793e-01,  3.0385e-01, -1.0948e-01]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.2423, -0.2516, -0.0125, -0.2999, -0.0071, -0.2103, -0.3241,  0.1905],\n",
       "          [ 0.2636, -0.1296, -0.2508, -0.3276,  0.2581, -0.3161,  0.3190, -0.0522],\n",
       "          [ 0.2760, -0.0331,  0.3144, -0.0349, -0.1993,  0.2363,  0.2485, -0.1021],\n",
       "          [ 0.0609,  0.1973, -0.1244,  0.2216,  0.3011, -0.3205, -0.1844, -0.0065],\n",
       "          [ 0.1024,  0.2791, -0.0539,  0.2222, -0.2359,  0.1300, -0.3129,  0.0444],\n",
       "          [ 0.3188, -0.1496,  0.1159,  0.2935, -0.1516, -0.1288,  0.3022, -0.2909],\n",
       "          [-0.3040,  0.2510, -0.2934,  0.3341,  0.1652, -0.1680,  0.0475, -0.3362],\n",
       "          [ 0.3198,  0.0643,  0.1986,  0.0802, -0.3071,  0.0961, -0.1122,  0.0232],\n",
       "          [ 0.1318, -0.0102, -0.0673,  0.0194, -0.2662,  0.1141, -0.2860, -0.3164],\n",
       "          [-0.2959,  0.1705,  0.0493, -0.0273,  0.2445,  0.2636,  0.2574, -0.0599],\n",
       "          [ 0.0629, -0.2665,  0.1934,  0.0931, -0.1410, -0.2923,  0.2673, -0.3189],\n",
       "          [-0.1493,  0.1283, -0.2241, -0.1044, -0.1557,  0.0755,  0.0362, -0.2013],\n",
       "          [ 0.1013, -0.0952, -0.3309, -0.2534, -0.2223,  0.0975,  0.2763,  0.2558],\n",
       "          [-0.0803, -0.3335,  0.0831,  0.3333,  0.0147, -0.2460, -0.2450,  0.0556],\n",
       "          [ 0.1898, -0.1481, -0.0864, -0.2111, -0.1176, -0.1814, -0.0144, -0.1534],\n",
       "          [ 0.0665,  0.0556,  0.3515,  0.2872,  0.1910, -0.0761,  0.1759,  0.3069],\n",
       "          [ 0.2449,  0.2403,  0.2418, -0.0817, -0.2138,  0.0144, -0.2553,  0.2804],\n",
       "          [-0.0449, -0.0820, -0.3138, -0.0156,  0.0303,  0.0853, -0.2547,  0.1836],\n",
       "          [ 0.3099,  0.0930,  0.2866, -0.3349,  0.1332, -0.3194, -0.0495,  0.3521],\n",
       "          [-0.2643,  0.2889,  0.2610,  0.2052, -0.2733,  0.3140,  0.2209, -0.2485],\n",
       "          [-0.2078, -0.1313, -0.0364, -0.1217,  0.1041,  0.0134, -0.0793, -0.1423],\n",
       "          [ 0.2582, -0.1434,  0.1763, -0.0022,  0.3036,  0.1030,  0.1099, -0.1517],\n",
       "          [ 0.1797,  0.3373, -0.3317,  0.1520, -0.1263, -0.0896, -0.1367,  0.0701],\n",
       "          [ 0.3439, -0.1783, -0.0597,  0.2806, -0.1553,  0.1323, -0.1528,  0.2632],\n",
       "          [-0.2913, -0.0201,  0.1363,  0.1766, -0.2125, -0.0094,  0.3341,  0.3027],\n",
       "          [ 0.2930,  0.2089,  0.3295, -0.1589, -0.1166,  0.3458,  0.0890,  0.1647],\n",
       "          [-0.0687,  0.2639,  0.2625, -0.2427, -0.1196,  0.1999,  0.0753, -0.2718],\n",
       "          [ 0.0570,  0.1945, -0.1146,  0.3241, -0.1979,  0.1620, -0.3385,  0.1819],\n",
       "          [ 0.1136,  0.3324, -0.2775, -0.0700,  0.0383, -0.1099, -0.2749,  0.1676],\n",
       "          [-0.1336,  0.3071, -0.2729,  0.1722, -0.0845,  0.2951, -0.2744,  0.1146],\n",
       "          [ 0.2466, -0.0474, -0.0201,  0.3281,  0.0530,  0.3527, -0.0292, -0.2595],\n",
       "          [-0.1044,  0.0447, -0.2562,  0.2572,  0.0114, -0.1943, -0.0984, -0.1846]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0058,  0.2348,  0.1226, -0.2635, -0.2061, -0.3309, -0.3338, -0.0052,\n",
       "          -0.3423, -0.2163,  0.1114, -0.0866, -0.2358, -0.0678, -0.2957, -0.3158,\n",
       "           0.0052, -0.2376, -0.1722,  0.2514, -0.1522, -0.3078,  0.0872,  0.2521,\n",
       "          -0.1265,  0.1232, -0.1690,  0.3198,  0.0016,  0.1964,  0.1601,  0.1386],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.2055, -0.1787, -0.3208,  0.0690, -0.0509, -0.2312, -0.0058, -0.3459,\n",
       "          -0.1902, -0.0636,  0.2054,  0.2148, -0.3198, -0.0361, -0.2987,  0.1549,\n",
       "          -0.2184,  0.3239, -0.1639, -0.3297, -0.2544, -0.1909, -0.3414,  0.0418,\n",
       "           0.2118,  0.3222, -0.0612, -0.2017,  0.2556,  0.0380,  0.0671, -0.0088],\n",
       "         requires_grad=True)],\n",
       " [Parameter containing:\n",
       "  tensor([[-2.8554e-01,  2.7479e-01, -2.0000e-01, -1.4232e-01, -6.7829e-02,\n",
       "           -2.5435e-01, -9.2419e-02, -6.7705e-02],\n",
       "          [-1.4912e-01,  3.1597e-01, -1.0166e-01,  2.2363e-01, -2.0468e-01,\n",
       "           -2.6831e-01,  7.3846e-02,  3.4208e-02],\n",
       "          [ 9.2460e-02, -2.0011e-01,  3.1363e-02,  1.4324e-01, -7.2284e-02,\n",
       "           -3.2949e-01,  3.4020e-01,  1.6381e-01],\n",
       "          [-2.2723e-01,  1.9364e-01, -3.0049e-01, -6.7127e-02,  2.4974e-01,\n",
       "           -4.7886e-04, -6.8637e-02,  3.7717e-02],\n",
       "          [-9.8468e-02,  1.1099e-01, -2.3023e-01,  9.0027e-02,  2.8174e-01,\n",
       "           -1.9228e-01,  2.5100e-01,  1.2625e-01],\n",
       "          [-1.9495e-01, -6.2665e-02, -3.3674e-01,  3.0760e-01,  3.4113e-01,\n",
       "           -1.4227e-01, -1.6802e-01, -1.0048e-01],\n",
       "          [ 3.2388e-01,  2.5424e-01,  1.2526e-01, -2.6160e-01,  1.3598e-01,\n",
       "            1.3521e-01,  2.9728e-02, -2.0356e-02],\n",
       "          [ 3.2529e-01,  1.0291e-01,  1.0631e-01, -3.8935e-02, -1.1150e-01,\n",
       "           -1.4355e-01,  3.8351e-02,  2.8866e-01],\n",
       "          [ 3.2671e-01,  1.5066e-01, -1.4668e-01, -2.8378e-01,  1.6525e-01,\n",
       "           -1.6594e-01,  2.9503e-01,  6.9854e-03],\n",
       "          [ 2.2466e-02,  2.3560e-01,  5.9390e-02,  9.8814e-02,  2.8179e-01,\n",
       "            2.5568e-01,  2.7798e-01,  2.7914e-02],\n",
       "          [ 2.4518e-01, -3.4492e-01,  1.6924e-01, -2.3921e-01,  1.0370e-01,\n",
       "           -2.7478e-01, -2.2087e-01,  3.9270e-02],\n",
       "          [ 1.3950e-02, -1.8862e-01,  3.2356e-01, -1.8902e-01, -1.8278e-01,\n",
       "            2.6808e-01, -2.5878e-01,  8.6750e-02],\n",
       "          [ 2.9047e-01,  8.9461e-02,  6.3304e-02, -2.6202e-01, -1.7785e-01,\n",
       "           -4.1515e-02, -1.5087e-02,  2.3869e-01],\n",
       "          [ 3.0416e-03,  9.6047e-02,  4.5339e-02,  2.1535e-01, -1.0788e-01,\n",
       "            3.1303e-01, -2.0938e-01, -7.1994e-02],\n",
       "          [ 1.2552e-01,  2.6326e-01,  2.8020e-01,  1.6159e-01, -1.7491e-01,\n",
       "           -4.3864e-02, -2.4909e-02,  2.4004e-01],\n",
       "          [-1.0008e-01, -2.3587e-02, -4.1141e-02,  1.7358e-01,  1.6116e-01,\n",
       "           -1.6301e-01,  3.3986e-01,  6.3661e-02],\n",
       "          [-9.1619e-03, -2.5549e-01,  2.6335e-02, -3.4696e-01,  1.6555e-01,\n",
       "           -2.4212e-01,  1.5773e-01, -3.2405e-01],\n",
       "          [ 4.3122e-03,  5.0682e-02,  3.4829e-01, -2.4392e-01, -3.0394e-01,\n",
       "            2.5682e-01,  1.7368e-01,  2.7552e-01],\n",
       "          [ 1.6481e-01,  8.6327e-02, -8.4835e-02, -2.5672e-01,  9.1428e-02,\n",
       "            1.8254e-01, -1.3164e-01, -3.3799e-01],\n",
       "          [-3.1158e-01, -1.0261e-01,  1.5507e-01, -1.7434e-01, -1.4167e-01,\n",
       "           -2.9037e-01, -8.9827e-02,  1.3489e-01],\n",
       "          [ 6.1781e-02,  2.0310e-04, -2.1370e-02,  1.4660e-01,  2.9498e-01,\n",
       "            1.6951e-01, -1.8998e-01,  3.3727e-01],\n",
       "          [-1.7311e-01, -1.0879e-01, -7.0193e-02,  3.5138e-01,  1.3386e-02,\n",
       "           -1.3983e-01,  8.1417e-02, -3.4705e-01],\n",
       "          [-1.8540e-01,  7.1958e-02,  8.2050e-02,  3.4164e-01,  6.5904e-02,\n",
       "           -3.9945e-02, -3.3492e-01, -1.6177e-01],\n",
       "          [-5.2556e-02,  1.0959e-01, -2.9717e-01,  9.7142e-02,  6.9507e-02,\n",
       "            2.1888e-01, -5.4339e-02, -3.3778e-01],\n",
       "          [ 1.5637e-01,  2.6607e-01, -1.6063e-01, -1.0521e-01,  7.3016e-02,\n",
       "            7.5102e-02, -3.2827e-01, -2.7235e-01],\n",
       "          [ 2.9966e-01,  2.7419e-03,  3.4759e-01,  1.6443e-01,  9.2726e-02,\n",
       "            5.9929e-02,  1.5746e-01, -2.3033e-01],\n",
       "          [ 1.2318e-01, -3.5337e-01,  1.3982e-01, -7.2516e-02, -9.0309e-02,\n",
       "            2.1964e-01,  3.8399e-02,  3.2488e-01],\n",
       "          [-2.9195e-01,  1.1346e-01,  1.4918e-01,  1.0236e-01,  6.9741e-02,\n",
       "            2.0074e-02,  4.9218e-02, -3.5018e-01],\n",
       "          [ 8.4268e-02, -3.1319e-01, -1.2222e-01,  1.1207e-01,  3.4089e-01,\n",
       "            3.2136e-01, -3.3544e-01,  3.2543e-01],\n",
       "          [-2.9135e-01, -1.8770e-01,  2.2215e-01, -1.7158e-01, -6.0533e-02,\n",
       "            2.0963e-01, -3.0298e-01, -1.6429e-01],\n",
       "          [-1.3606e-01, -3.4514e-01, -6.2761e-03, -1.2266e-01,  2.6686e-01,\n",
       "           -1.1437e-01, -2.0256e-02,  3.4153e-01],\n",
       "          [-2.4193e-02,  3.3708e-01, -4.7554e-02,  2.4972e-01, -3.4674e-01,\n",
       "           -9.7186e-03, -3.3101e-01, -1.1439e-01]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 2.4587e-01,  2.5341e-01, -1.3325e-01, -1.9536e-01, -1.0182e-01,\n",
       "           -3.4967e-01, -6.4247e-02,  1.0288e-01],\n",
       "          [-3.4065e-01, -3.0421e-01,  1.3455e-02, -2.0092e-01,  1.0531e-02,\n",
       "           -1.5850e-01,  2.8498e-01,  2.4453e-01],\n",
       "          [-3.1378e-01, -3.5139e-01, -2.9681e-01,  3.0992e-01,  1.4401e-01,\n",
       "           -1.7643e-01, -3.1631e-02, -1.7662e-01],\n",
       "          [ 3.7995e-02,  1.0034e-01,  1.6457e-01,  5.1365e-02, -1.8359e-01,\n",
       "            4.8579e-02, -3.3977e-01,  9.4561e-02],\n",
       "          [-3.3610e-02, -5.6115e-02,  5.1261e-02,  1.3428e-02, -3.1211e-01,\n",
       "            3.0210e-01,  1.3673e-01, -1.8395e-01],\n",
       "          [-2.9016e-01,  3.4755e-01,  9.6558e-02,  2.4201e-02, -2.5940e-01,\n",
       "           -1.7970e-01, -3.5326e-01,  8.1168e-02],\n",
       "          [ 1.4726e-01,  7.3493e-02,  3.1014e-01,  1.9637e-01, -2.0398e-01,\n",
       "            2.7430e-01, -1.3434e-01, -4.0697e-02],\n",
       "          [-1.4649e-01, -2.0525e-01,  2.4251e-01, -1.3790e-02, -6.8523e-02,\n",
       "            1.1270e-01,  1.4716e-01, -1.4661e-01],\n",
       "          [ 1.8178e-01, -1.9824e-01, -1.5588e-01,  5.1006e-02, -8.2521e-02,\n",
       "            1.9477e-01,  3.0863e-01,  2.1096e-01],\n",
       "          [-2.0725e-01,  2.8232e-01,  1.6548e-01, -2.5480e-01,  2.1980e-01,\n",
       "           -2.5130e-01,  1.1667e-01, -1.8850e-01],\n",
       "          [ 3.0767e-01, -7.2847e-02, -1.4709e-02,  1.2708e-02,  1.4298e-01,\n",
       "           -1.6116e-01, -1.4866e-01, -5.3313e-02],\n",
       "          [ 3.1606e-01, -3.4639e-01, -2.1583e-01,  9.3733e-02,  4.1939e-02,\n",
       "            6.0071e-02, -9.4577e-02,  1.9883e-01],\n",
       "          [-5.0618e-02,  9.4065e-02,  2.9565e-01, -1.0296e-01,  6.0453e-02,\n",
       "            1.5736e-01, -2.9805e-01, -2.3696e-01],\n",
       "          [-3.3776e-01,  2.6457e-01, -5.8503e-02, -2.5790e-01,  1.2025e-01,\n",
       "           -1.7374e-01, -1.4556e-01,  6.7300e-02],\n",
       "          [ 3.3835e-01,  1.6468e-01, -2.6513e-01, -2.4907e-01,  2.3203e-01,\n",
       "           -1.7235e-01,  1.8774e-01, -1.4641e-01],\n",
       "          [ 2.9913e-01, -6.1623e-02, -8.3123e-03, -3.4541e-01, -1.6136e-01,\n",
       "           -2.7319e-02, -3.5270e-02, -3.2186e-01],\n",
       "          [-3.6848e-02,  2.5924e-01,  2.0171e-01, -2.2351e-01, -1.8848e-01,\n",
       "           -1.8357e-01,  1.1791e-01, -3.1876e-01],\n",
       "          [-3.3860e-01,  3.0100e-01, -1.6068e-01,  1.0789e-01, -1.6688e-01,\n",
       "           -6.3500e-03,  2.8141e-01, -2.9437e-01],\n",
       "          [-3.2722e-01, -1.6780e-01, -4.4162e-02, -1.2287e-01, -7.2689e-02,\n",
       "            2.0745e-01,  1.3629e-01,  4.8578e-02],\n",
       "          [ 2.1560e-01,  3.3898e-01,  2.0274e-01, -1.0146e-01, -2.9884e-01,\n",
       "            1.7400e-01, -2.4218e-01, -3.2461e-01],\n",
       "          [-1.8722e-01,  2.4104e-01, -6.1760e-02, -1.1189e-01, -1.4520e-01,\n",
       "           -1.1249e-01,  3.4107e-01,  1.8817e-01],\n",
       "          [ 5.8805e-03, -3.1078e-01, -1.2812e-01, -1.3881e-01,  1.3296e-01,\n",
       "            6.2631e-02, -2.5186e-02, -2.1936e-01],\n",
       "          [ 2.8519e-01, -2.4134e-02, -2.2799e-01,  1.1061e-01, -3.0708e-01,\n",
       "           -1.2941e-01, -1.2240e-04,  2.6004e-01],\n",
       "          [ 1.0870e-01,  2.9258e-01, -2.4114e-01, -2.9709e-01, -1.0778e-01,\n",
       "            8.8244e-02, -3.4989e-01,  1.6966e-01],\n",
       "          [ 1.9459e-02,  2.5788e-01, -1.6767e-01, -4.8240e-02,  3.1864e-01,\n",
       "           -2.9160e-01,  1.8731e-02, -3.3518e-01],\n",
       "          [ 1.0193e-01,  7.9310e-02,  3.2633e-01,  3.2852e-01,  5.7108e-02,\n",
       "            1.4700e-01, -7.1690e-02,  1.8476e-01],\n",
       "          [ 3.4704e-01, -6.8389e-02, -1.1247e-01, -2.2038e-01, -1.5213e-01,\n",
       "           -2.6926e-02, -5.8497e-03,  2.1924e-01],\n",
       "          [ 3.7856e-02,  8.4222e-02,  2.3684e-02,  1.2811e-02, -4.7142e-03,\n",
       "           -5.3322e-03, -2.6725e-01,  3.0509e-01],\n",
       "          [-3.2550e-01,  3.0706e-01, -3.3009e-01, -1.5229e-01, -2.3180e-01,\n",
       "            1.0345e-01,  1.7562e-01, -1.6132e-01],\n",
       "          [ 2.3471e-01, -7.6970e-02, -1.2686e-01, -1.3948e-01, -1.7280e-01,\n",
       "           -3.3250e-01, -3.2377e-01, -3.2693e-01],\n",
       "          [ 8.8468e-02,  1.4931e-01, -2.2163e-01,  2.4658e-01, -2.1547e-01,\n",
       "           -1.7436e-01,  1.0482e-01, -2.7618e-01],\n",
       "          [ 3.4327e-02, -6.4321e-02, -2.4516e-01,  2.0082e-01, -2.0779e-01,\n",
       "           -1.7665e-01, -1.5728e-01,  3.1546e-01]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.2876, -0.0941,  0.0884,  0.0845, -0.2158, -0.2209, -0.2207,  0.1521,\n",
       "          -0.2948,  0.0349,  0.2722, -0.1463, -0.1700, -0.1797,  0.1179, -0.2170,\n",
       "          -0.2553, -0.0983, -0.1833, -0.2502,  0.2978,  0.2939, -0.2560, -0.0678,\n",
       "           0.0812,  0.2700,  0.0917,  0.3144,  0.2400, -0.1897, -0.1211, -0.2814],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1738,  0.0691,  0.3445,  0.2992,  0.1087, -0.0994,  0.3388, -0.2619,\n",
       "          -0.0881, -0.2966,  0.1364,  0.1087, -0.0218,  0.3370, -0.1753,  0.1643,\n",
       "          -0.1885, -0.1037, -0.2532, -0.2446,  0.2545, -0.1142,  0.0569, -0.2784,\n",
       "          -0.0638, -0.0534, -0.3221,  0.0304, -0.3297,  0.2374, -0.3162, -0.2198],\n",
       "         requires_grad=True)]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lstm.all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.char_embeddings(input)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output.view(1, 1, -1), hidden.view(1, 1, 1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.char_embeddings(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output.view(1, 1, -1), hidden.view(1, 1, 1, -1))\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, dropout_p=0.1, max_length=max_len):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.char_embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.attn = nn.Linear(self.embedding_dim * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.embedding_dim * 2, self.embedding_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.LSTM(self.embedding_dim, self.hidden_dim)\n",
    "        self.out = nn.Linear(self.hidden_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.char_embeddings(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden.view(1, 1, 1, -1))\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=max_len):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_dim, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = list(zip(data, labels))[:n_iters]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def evaluate(encoder, decoder, sentence, max_length=max_len):\n",
    "#     with torch.no_grad():\n",
    "#         input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "#         input_length = input_tensor.size()[0]\n",
    "#         encoder_hidden = encoder.initHidden()\n",
    "\n",
    "#         encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "#         for ei in range(input_length):\n",
    "#             encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "#                                                      encoder_hidden)\n",
    "#             encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "#         decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "#         decoder_hidden = encoder_hidden\n",
    "\n",
    "#         decoded_words = []\n",
    "#         decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "#         for di in range(max_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             decoder_attentions[di] = decoder_attention.data\n",
    "#             topv, topi = decoder_output.data.topk(1)\n",
    "#             if topi.item() == EOS_token:\n",
    "#                 decoded_words.append('<EOS>')\n",
    "#                 break\n",
    "#             else:\n",
    "#                 decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "#             decoder_input = topi.squeeze().detach()\n",
    "\n",
    "#         return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-c3eab844c155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-67be3327d39b>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 18\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-f7486532c820>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         encoder_output, encoder_hidden = encoder(\n\u001b[0;32m---> 19\u001b[0;31m             input_tensor[ei], encoder_hidden)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-b41ad6b16af3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    493\u001b[0m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[1;32m    494\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[0;32m--> 495\u001b[0;31m         self.check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[0m\u001b[1;32m    496\u001b[0m                                'Expected hidden[1] size {}, got {}')\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMBEDDING_DIM = 8\n",
    "HIDDEN_DIM = 8\n",
    "encoder1 = EncoderRNN(EMBEDDING_DIM, HIDDEN_DIM, len(char_emb))\n",
    "attn_decoder1 = AttnDecoderRNN(EMBEDDING_DIM, HIDDEN_DIM, len(char_emb), dropout_p=0.1)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, len(data), print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
